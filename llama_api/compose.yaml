services:
  # -------------------------------------------------------------------- 
  # 2) Llama API  https://hub.docker.com/r/jflachman/llama-cpp-python   
  # --------------------------------------------------------------------
  llama_api:
    image: jflachman/llama-cpp-python:v0.2.77-cuda  # ghcr.io/ggerganov/llama-cpp-python:v0.2.77-cuda # or cpu
    container_name: llama_api
    #restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      #- USE_MLOCK=0
      - MODEL=/models/Phi-3_finetuned.gguf
      - PORT=11434
      - N_CTX=2048
      - N_THREADS=12
      - N_GPU_LAYERS=-1
    #volumes:
    #  - .../deployment-files/models:/models
    #  - ./llama_api:/config
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
    networks: [appnet]
    #command: ["python3", "-m", "llama_cpp.server", "--config", "/config/server.config"]
    command: >
      sh -c "
        python3 -m llama_cpp.server --help &&
        echo '\\nChecking volumes' &&
        ls &&
        echo '\\nCHecking models volume' &&
        ls models/ &&
        echo '\\nStarting actual model now' &&
        python3 -m llama_cpp.server --model models/Phi-3_finetuned.gguf --n_gpu_layers -1 --n_threads 12 --n_ctx 2048 --verbose true &&
        echo 'LLaMA server started'
      "
  
    